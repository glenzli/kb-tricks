# kb-tricks 技能套件评测方案 (Benchmark Proposal)

为了科学地评测 `kb-build` 及其衍生技能（如 `kb-update`, `moe-cr`）的有效性，我们需要结合**静态制品检查**和**动态检索测试**，构建一条完整的评测 Benchmark 链路。

## 1. 测试基准库准备 (Baseline Repositories)

选择 2-3 个复杂度适中、但具有不同架构范式的开源项目作为测试床：
1. **典型后端服务**：如基于 Express / FastAPI / Spring Boot 的项目。用于测试系统对 API 契约、数据流和状态转换的理解。
2. **典型前端/客户端项目**：如小型的 React / Vue / Flutter 项目。用于测试系统对组件树、状态管理和生命周期的理解。
3. **工具/库类项目**：无外部服务依赖的纯逻辑代码库，测试核心算法和边界条件提取。

## 2. 三维评测维度

### 维度一：静态资产检查 (Static Quality Assessment)

在目标仓库上完整运行 `kb-build` 技能，然后对生成的知识库目录（如 `.agent/kb/`）进行自动化或人工评估：

*   **信噪比 (Signal-to-Noise Ratio)**：评估是否有效过滤了样板代码（如纯粹的 Get/Set，无逻辑的 DTO），是否提炼出了模块间的核心交互关系（认知地图）。
*   **合规率 (Compliance Rate)**：
    *   检查是否 100% 的生成文件都在 Frontmatter 头部包含了规范的 YAML `fingerprint`（包含关联的源码文件路径和 Commit ID）。
    *   检查 Commit ID 提取的准确性和时效性。
*   **格式可用性 (Syntax Validity)**：
    *   验证所有 Mermaid 图表语法是否合法且可渲染。
    *   验证 Markdown 相对链接是否连通（无死链），以及是否遵守了单一事实来源（SSOT）原则。

### 维度二：隔离问答能力审核 (Isolated RAG Audit)

这是测试该技能是否实现了“认知地图”初衷的最核心手段。测试系统生成的知识图谱是否真正有用，且没有幻觉：

*   **评测过程**：
    1. 启动一个**完全没有源码访问权限**的全新 Agent 实例（仅将其上下文挂载于生成的知识库目录）。
    2. 基于项目的业务和技术难点，向 Agent 提出 3 个设计维度的“刁钻”问题（如：如何安全地扩展某个接口而不破坏依赖、某段核心逻辑的异常边界在哪）。
*   **评估指标**：
    *   **准确率 (Accuracy)**：回答的解决思路是否正确。
    *   **溯源率 (Citation Rate)**：回答中是否明确引用了具体的知识库文档（如 `xxx.md`）及相应的段落/行号，证明答案来于知识库而非通用知识。
    *   **盲点识别率 (Blindspot Rate)**：面对源码中有但在构建阶段未被知识库记录的细节，Agent 是否能诚实地指出这是“知识库盲区（Blindspot）”而不是捏造幻觉（Hallucination）。

### 维度三：抗衰退与可维护性测试 (Resilience & Maintainability Test)

测试构建的知识链路是否具备长期的生命力，此步需要联动 `kb-update` 进行验收：

*   **评测过程**：
    1. 在基准测试仓库中提交一个“破坏性”的改动（如修改核心中间件的方法签名、移除某个组件或重构核心数据表结构）。
    2. 触发 `kb-update` 技能执行增量维护逻辑。
*   **评估指标**：
    *   **精准定位 (Precision Navigation)**：系统能否根据 `kb-build` 留下的 Git 文件指纹，直接命中受影响的 1~2 个过时文档，而不是重写或重新通读大量无关文档。
    *   **级联一致性 (Cascade Consistency)**：系统能否沿着已建立的 Markdown 知识图谱链接（如 SSOT 内部引用），顺藤摸瓜更新受影响的下游文档（例如：核心 API 的改动自动反馈到了鉴权文档的补充说明中）。
